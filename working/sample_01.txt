In a study published on May 21, 2022, researchers from Anthropic investigated the impact of repeated data in large language models. The study focused on understanding how repeated exposure to certain data points affects model performance and interpretability.

Key findings include:
- A strong double descent phenomenon was observed, where repeating data can initially improve test loss but then lead to an increase in test loss midway through training.
- Even a small fraction of repeated data (0.1% in some cases) could significantly degrade model performance, reducing the performance of an 800 million parameter model to that of a 400 million parameter model.
- The researchers hypothesize that there is an optimal range where the model memorizes the repeated data, consuming a large portion of its capacity and leading to the observed degradation in performance.

The study also connected these observations to recent mechanistic interpretability work by showing that repeated data disproportionately damages copying mechanisms and internal structures related to generalization. This suggests that such repetition shifts the model's behavior from generalizing to memorizing specific examples.

These findings provide insights into why repeating a small fraction of data can have significant negative effects on large language models, potentially leading to substantial performance declines.